{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUStugfzWud2"
      },
      "source": [
        "## CSCE 676 :: Data Mining and Analysis :: Texas A&M University :: Spring 2022\n",
        "\n",
        "\n",
        "# Homework 2\n",
        "\n",
        "- **100 points [7% of your final grade]**\n",
        "- **Due Sunday, March 6 by 11:59pm** \n",
        "- (*no submissions accepted after March 9 by 11:59pm*)\n",
        "\n",
        "**Goals of this homework:** There are four objectives of this homework: \n",
        "\n",
        "* Become familiar with Apache Spark;\n",
        "* Get hands-on experience using Spark built-in functions like PageRank;\n",
        "* Work out Hubs and Authorities by hand;\n",
        "* Try some LSH problems.\n",
        "\n",
        "*Submission instructions:* You should post your notebook to canvas (look for the homework 2 assignment there). Name your submission **your-uin_hw2.ipynb**, so for example, my submission would be something like **555001234_hw2.ipynb**. Your notebook should be fully executed when you submit ... so run all the cells for us so we can see the output, then submit that. When you are done, download your notebook from colab and submit it to canvas.\n",
        "\n",
        "*Collaboration declaration:* If you worked with someone on this homework, please be sure to mention that. Remember to include citations to any sources you use in the homework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eql4ThhZt5lL"
      },
      "source": [
        "*Write your collaboration/references here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AQt_pCoWueB"
      },
      "source": [
        "## Preliminaries: Introduction to the Dataset\n",
        "We will use a dataset of tweets concerning members of the US congress. The data spans almost a year (from October 3rd, 2018 to September 25th, 2019) covering 576 of the members. Any tweet or retweet posted by the 576 members or directed to them by other Twitter users were collected.\n",
        "\n",
        "Originally there were more than 200 million tweets collected but we have sampled 400,000 tweets for this homework.\n",
        "\n",
        "Below is a summary of all datasets used for this homework:\n",
        "\n",
        "| Dataset                        | Description |\n",
        "| :---                                                       | :---\n",
        "| Congress members               | 576 twitter ids and screen names |\n",
        "| Sample tweets                  |400k sample tweets|\n",
        "| ~~User hashtags~~                  | ~~all pairs of <user, hashtag>~~|\n",
        "| User mentions                  | all pairs of <src_user_id, src_dest_id, frequency> |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ8cZb5cg5Im"
      },
      "source": [
        "Following are the steps that you need to execute to install spark. Remember you need to execute them everytime after your runtime is disconnected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xiQ6bMbXITQ"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "!pip install graphframes\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!curl -L -o \"/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.1-spark3.0-s_2.12.jar\" https://repos.spark-packages.org/graphframes/graphframes/0.8.1-spark3.0-s_2.12/graphframes-0.8.1-spark3.0-s_2.12.jar\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHtukwBPl9Nv"
      },
      "source": [
        "The cell below will download the dataset from S3 to us-congress-tweets directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWGZu0fZ5Yv9"
      },
      "outputs": [],
      "source": [
        "!mkdir us-congress-tweets\n",
        "!wget https://us-congress.s3.amazonaws.com/congress_members.csv -O us-congress-tweets/congress_members.csv\n",
        "!wget https://us-congress.s3.amazonaws.com/tweets-sample-400k.json -O us-congress-tweets/tweets-sample-400k.json\n",
        "!wget https://us-congress.s3.amazonaws.com/user_hashtags-1m.csv -O us-congress-tweets/user_hashtags-1m.csv\n",
        "!wget https://us-congress.s3.amazonaws.com/user_mentions.csv -O us-congress-tweets/user_mentions.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWOQNZBaXPgs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark import SparkContext, SparkConf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG8qQdgRXViL"
      },
      "outputs": [],
      "source": [
        "# create a Spark session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create a Spark context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6qAn1XJWueC"
      },
      "outputs": [],
      "source": [
        "# First let's read Twitter ids and screen names of the 576 US congress members\n",
        "\n",
        "congress_members = spark.read.csv(\"us-congress-tweets/congress_members.csv\", header=True)\n",
        "congress_members.show()\n",
        "print(\"Number of congress members tracked:\", congress_members.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOfjGbB6WueF"
      },
      "source": [
        "We can use `spark.read.json(...)` without schema to load the tweets into a dataframe but this will be slow for two reasons:\n",
        "* First, it will make one pass over the data to build a schema of the content, then a second pass to read the content and parse it to the dataframe. \n",
        "* It will read all the content of the Tweet JSON objects but we only need few fields for a given task.\n",
        "\n",
        "Thus we define our own schema something like the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLBnQUluWueG"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "twitter_date_format=\"EEE MMM dd HH:mm:ss ZZZZZ yyyy\"\n",
        "\n",
        "user_schema = StructType([\n",
        "    StructField('created_at', TimestampType(), True),\n",
        "    StructField('followers_count', LongType(), True),\n",
        "    StructField('id', LongType(), True),\n",
        "    StructField('name', StringType(), True),\n",
        "    StructField('screen_name', StringType(), True)\n",
        "])\n",
        "\n",
        "hashtag_schema = ArrayType(StructType([StructField('text', StringType(), True)]))\n",
        "user_mentions_schema = ArrayType(StructType([StructField('id', LongType(), True),\n",
        "                                             StructField('screen_name', StringType(), True)]))\n",
        "entities_schema = StructType([\n",
        "    StructField('hashtags', hashtag_schema, True),\n",
        "    StructField('user_mentions', user_mentions_schema, True)\n",
        "    ])\n",
        "\n",
        "retweeted_status_schema =StructType([        \n",
        "        StructField(\"id\", LongType(), True),\n",
        "        StructField(\"in_reply_to_user_id\", LongType(), True),\n",
        "        StructField(\"in_reply_to_status_id\", LongType(), True),\n",
        "        StructField(\"created_at\", TimestampType(), True),\n",
        "        StructField(\"user\", user_schema)\n",
        "    ])\n",
        "\n",
        "tweet_schema =StructType([\n",
        "        StructField(\"text\", StringType(), True),\n",
        "        StructField(\"id\", LongType(), True),\n",
        "        StructField(\"in_reply_to_user_id\", LongType(), True),\n",
        "        StructField(\"in_reply_to_status_id\", LongType(), True),\n",
        "        StructField(\"created_at\", TimestampType(), True),\n",
        "        StructField(\"user\", user_schema),\n",
        "        StructField(\"entities\", entities_schema),\n",
        "        StructField(\"retweeted_status\", retweeted_status_schema)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6xAE5NXWueH"
      },
      "source": [
        "Now we are ready to read the tweets with `spark.read.json` passing our own schema as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZa4FIDAWueH"
      },
      "outputs": [],
      "source": [
        "tweets = spark.read.option(\"timestampFormat\", twitter_date_format)\\\n",
        "                   .json('us-congress-tweets/tweets-sample-400k.json', tweet_schema)\\\n",
        "                   .withColumn('user_id',F.col('user.id'))\n",
        "tweets.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyAc0ou-WueI"
      },
      "source": [
        "## (30 points) Part 1: Let's Fire up Spark for Real\n",
        "\n",
        "Okay, now that we've finally got our data loaded, we are ready to get our hands dirty with Spark. As you know, Spark comes with lots of built-in functions and nice capabilities. Indeed, you will find that to answer the problems below, there are **many** possible approaches. Please refer to the Stanford Spark tutorial we posted alongside this homework. Also feel free to Google around for some Spark resources, but we list a few here to get you started:\n",
        "\n",
        "* https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#aggregate-functions\n",
        "* https://spark.apache.org/docs/latest/sql-getting-started.html#running-sql-queries-programmatically\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFo2xXh1WueI"
      },
      "source": [
        "### Exploratory Data Analysis\n",
        "\n",
        "a) How many unique users are there? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZJGG4bzWueK"
      },
      "outputs": [],
      "source": [
        "# your code here for unique users"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY5_nDCCWueM"
      },
      "source": [
        "b) How many times is the user **GOPLeader** mentioned?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrkN__kuWueM"
      },
      "outputs": [],
      "source": [
        "# code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIwGhG_OWueN"
      },
      "source": [
        "c) Now find the top-5 most mentioned members of Congress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fztHxTpyWueN"
      },
      "outputs": [],
      "source": [
        "# code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGuKUpyqZ2ZF"
      },
      "source": [
        "d) For each month in the dataset, report the top-5 most mentioned users (across all users in the dataset, not just the members of Congress). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgdYVF2cZ2un"
      },
      "outputs": [],
      "source": [
        "# code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51iGNYmNaUW1"
      },
      "source": [
        "e) Do you notice anything interesting in your monthly analysis?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSYkjX5QaYIf"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha2vrLOmWueh"
      },
      "source": [
        "## (25 points) Part 2: PageRank\n",
        "Now let's find out who are the most important users in the dataset by running PageRank over the mentions network. For this problem, we'll define a user-mentions network using relations in `us-congress-tweets/user_mentions.csv`. That is, each user is a node in the graph. If user A mentions user B, then there should be an edge from A to B.\n",
        "\n",
        "Hint:\n",
        "* Take a look at GraphFrames: https://graphframes.github.io/graphframes/docs/_site/user-guide.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1HwmZJCWueh"
      },
      "outputs": [],
      "source": [
        "# your network construction code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg0R72ymwiyK"
      },
      "source": [
        "How many vertices are there in your network? And how many edges?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0_aNONPwotS"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WMaKjp5Zlb8"
      },
      "source": [
        "Now you can run PageRank on the constructed network. Feel free to use the implementation provided as part of GraphFrames. Use a reset (or teleportation probability) of 0.15. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHgs08vfWuei"
      },
      "outputs": [],
      "source": [
        "# your PageRank code here (should be pretty short since you're just calling a function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeWYZx8RZz9n"
      },
      "source": [
        "OK, let's see what we found. List the top-10 users based on the PageRank values calculated above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMtXqwdDcS0V"
      },
      "outputs": [],
      "source": [
        "# Top 10 accounts "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuX2j4A-aFvz"
      },
      "source": [
        "What do you observe? are the top 10 users congress members? are there outsiders?\n",
        "\n",
        "Hint: You can use https://twitter.com/intent/user?user_id=? to find out more info about the users"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN7PbajvzOea"
      },
      "source": [
        "*your analysis here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUcdJqltBYyf"
      },
      "source": [
        "## (5 points) Bonus Question (Optional)\n",
        "Until now you have explored different aspects of Spark to investigate tweets by the US Congress. For this (optional) bonus problem, please conduct your own investigation of the data. You can use any combinations of the data files to carry out your investigation. You may use any libraries in Spark that you like, but you must use Spark. Tell us what question motivates you, what you did, what you discovered, and why it is interesting. This is completely open-ended, but we are looking for compelling questions and insights. Good luck!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0DTKHMRDlkt"
      },
      "source": [
        "Feel free to add any number of code/text cells here for your answer!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK7_5Q-4Djg1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqpssb-yE8a"
      },
      "source": [
        "## (20 points) Hubs and Authorities by Hand\n",
        "\n",
        "Consider the following graph (in adjacency list format):\n",
        "\n",
        "* A: B, D (that is, A links to B and to D)\n",
        "* B: D, E\n",
        "* C: D, E\n",
        "* D: F, G\n",
        "* E: C, D, G, H\n",
        "* F: \n",
        "* G: \n",
        "* H: C, G\n",
        "\n",
        "Find the hub and authority score for each node. Please normalize your final scores so they sum to 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4vm-2xxy6qt"
      },
      "source": [
        "*show your steps here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hxh70yrxS50"
      },
      "source": [
        "## (25 points) LSH Problems from the Book\n",
        "\n",
        "From MMDS [Chapter 3](http://infolab.stanford.edu/~ullman/mmds/ch3n.pdf), do the following problems:\n",
        "\n",
        "* Exercise 3.1.2 (Page 78)\n",
        "* Exercise 3.2.2 (Page 81)\n",
        "* Exercise 3.3.3 (Page 90)\n",
        "* Exercise 3.4.1 (Page 96)\n",
        "* Exercise 3.4.2 (Page 96)\n",
        "\n",
        "Please add cells below with your answers (explanation and detail of calculation are required). \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "homework_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}